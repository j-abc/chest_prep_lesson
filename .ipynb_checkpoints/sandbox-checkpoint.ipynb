{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model look up table\n",
    "# folder convention:\n",
    "# modelid/\n",
    "    # modelid_epoch%d.h5\n",
    "    # modelid_history_log.csv\n",
    "    \n",
    "# FOR ALL: 20 epochs\n",
    "\n",
    "# models of interest:\n",
    "# logistic\n",
    "    # best model saved\n",
    "    \n",
    "# fcc: 2 layer\n",
    "    # best model saved\n",
    "    \n",
    "# cnn_lay%d_drop%d_learn%d_aug%d -- \n",
    "    # layers: 2, 4, 6\n",
    "    # dropout: 1e-x: 0, 3, 6\n",
    "    # rate: 0, 0.3, 0.6\n",
    "    # 27 models: \n",
    "    # 0.3 hour per model\n",
    "    # 27 best models saved\n",
    "    # augmentation ONLY for the best model\n",
    "\n",
    "# vgg_pretrained: 36 minutes for 20 epochs\n",
    "    # 900 ms/step\n",
    "    # 0.6 hour per model\n",
    "\n",
    "# 15 gb of models for the imaging notebook\n",
    "\n",
    "# base model function \n",
    "# fcc\n",
    "# cnn(number_layers, dropout, learning_rate, uses_augmentation )\n",
    "# 2, 4, 6\n",
    "# 0, 0.3, 0.6\n",
    "# learning_rate: -6, -4, -2\n",
    "\n",
    "# for the best one, check against augmentation \n",
    "    # uses augmentation, doesn't use augmentation \n",
    "\n",
    "# 2 models to save for each : last version and the best epoch\n",
    "# for the best model...\n",
    "\n",
    "# what i'll do for each of these... is... \n",
    "# (1) prepare_model_database on a single instance\n",
    "# (2) port model_database onto all other instances\n",
    "# (3) run 8 rows per instance\n",
    "# (4) once finished, aggregate all data into the google drive\n",
    "    # all model folders will have 'model_' as their prefix\n",
    "# (5) prepare notebook GUI for CNN to display...\n",
    "    # parameters\n",
    "    # training curve, validation curve (losses + accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, model_name, augment, results_dir, run_now = True):\n",
    "    '''\n",
    "    takes in...\n",
    "    runs our model\n",
    "    '''\n",
    "    save_dir = results_dir + model_name + '/'\n",
    "    print('Results directory: %s'%save_dir)\n",
    "    ensure_directory(save_dir)\n",
    "    \n",
    "    print('Running model: %s'%model_name)\n",
    "    print('\\tDefining generators...')\n",
    "\n",
    "    # load our generators \n",
    "    train_generator = get_train_data_generator(augment = augment)\n",
    "    val_generator   = get_val_data_generator()\n",
    "    \n",
    "    print('\\tFitting model...')\n",
    "    # run our model\n",
    "    if run_now:\n",
    "        # define our callbacks\n",
    "        callbacks = get_callbacks(model_name, save_dir)\n",
    "        \n",
    "        model.fit_generator(train_generator,\n",
    "                            validation_data=val_generator, \n",
    "                            validation_steps = 1, \n",
    "                            steps_per_epoch  = 1, \n",
    "                            epochs = 1,\n",
    "                            callbacks=callbacks)\n",
    "        print('\\tSaving model...')        \n",
    "        model.save(save_dir + model_name + '_end.h5')\n",
    "        return 'Ran!'\n",
    "    return 'Dry!'\n",
    "\n",
    "def prepare_model_name(model_name, augment):\n",
    "    '''\n",
    "    one off... \n",
    "    '''\n",
    "    # if augmented\n",
    "    if augment:\n",
    "        model_name = model_name + '_aug'        \n",
    "    return model_name\n",
    "\n",
    "def choose_model(model_type, params):\n",
    "    '''\n",
    "    takes in model_type and the parameters as a dict\n",
    "    returns model and its defined name \n",
    "    '''\n",
    "    model_dict = {'log':get_log_model,\n",
    "                  'fcc':get_fcc_model,\n",
    "                  'cnn':get_cnn_model,\n",
    "                  'vgg':get_vgg_model}\n",
    "    return model_dict[model_type](**params)\n",
    "\n",
    "def prepare_models_list():\n",
    "    '''\n",
    "    takes in nothing\n",
    "    returns list of models that we want to sweep across\n",
    "    '''\n",
    "    \n",
    "    models_list = []\n",
    "    models_list.append(['log', {}, {'augment': 0}])\n",
    "    models_list.append(['fcc', {}, {'augment': 0}])\n",
    "    \n",
    "    # cnn models\n",
    "    params = [(ilay, idrop, ilr) for ilay in [1,2,4] for idrop in [0, 3, 6] for ilr in [0, 3, 6]]\n",
    "    for (ilay, idrop, ilr) in params:\n",
    "        models_list.append(['cnn',{'num_layers':ilay, 'dropout':idrop,'learning_rate':ilr}, {'augment':0}])\n",
    "        # layers: 1, 2, 4\n",
    "        # dropout: 1e-x: 0, 3, 6\n",
    "        # rate: 0, 0.3, 0.6\n",
    "    \n",
    "    # vgg\n",
    "    for trainable in [1, 0]:\n",
    "        models_list.append(['vgg', {'trainable':trainable}, {'augment':1}])\n",
    "    return models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = prepare_models_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_iter(run_row, run_now = False):\n",
    "    model_type, model_params, run_params = run_row\n",
    "    model, model_name = choose_model(model_type, model_params)\n",
    "    model_name = prepare_model_name(model_name, **run_params)\n",
    "    _ = run_model(model = model, model_name = model_name, **run_params, run_now = run_now, results_dir = '/home/jupyter/models/')\n",
    "    reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results directory: /home/jupyter/models/logistic/\n",
      "Running model: logistic\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 821ms/step - loss: 0.6889 - acc: 0.4688 - val_loss: 7.4949 - val_acc: 0.5350\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/fcc/\n",
      "Running model: fcc\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6153 - acc: 0.8438 - val_loss: 8.7844 - val_acc: 0.4550\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop0_lr0/\n",
      "Running model: cnn_lay1_drop0_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6637 - acc: 0.6875 - val_loss: 7.5755 - val_acc: 0.5300\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop0_lr3/\n",
      "Running model: cnn_lay1_drop0_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6951 - acc: 0.4375 - val_loss: 0.7555 - val_acc: 0.5050\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop0_lr6/\n",
      "Running model: cnn_lay1_drop0_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.6969 - acc: 0.4688 - val_loss: 0.6940 - val_acc: 0.5200\n",
      "\tSaving model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop3_lr0/\n",
      "Running model: cnn_lay1_drop3_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.7261 - acc: 0.3750 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop3_lr3/\n",
      "Running model: cnn_lay1_drop3_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.6877 - acc: 0.5312 - val_loss: 0.6842 - val_acc: 0.5950\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop3_lr6/\n",
      "Running model: cnn_lay1_drop3_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.7158 - acc: 0.3750 - val_loss: 0.7006 - val_acc: 0.4800\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop6_lr0/\n",
      "Running model: cnn_lay1_drop6_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.6894 - acc: 0.5312 - val_loss: 7.8173 - val_acc: 0.5150\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop6_lr3/\n",
      "Running model: cnn_lay1_drop6_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.7117 - acc: 0.4375 - val_loss: 0.7115 - val_acc: 0.4950\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay1_drop6_lr6/\n",
      "Running model: cnn_lay1_drop6_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.6830 - acc: 0.5938 - val_loss: 0.6935 - val_acc: 0.4500\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop0_lr0/\n",
      "Running model: cnn_lay2_drop0_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.6945 - acc: 0.5000 - val_loss: 8.4620 - val_acc: 0.4750\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop0_lr3/\n",
      "Running model: cnn_lay2_drop0_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.6896 - acc: 0.5625 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop0_lr6/\n",
      "Running model: cnn_lay2_drop0_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7085 - acc: 0.4375 - val_loss: 0.7016 - val_acc: 0.4950\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop3_lr0/\n",
      "Running model: cnn_lay2_drop3_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7023 - acc: 0.4688 - val_loss: 2.5537 - val_acc: 0.4550\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop3_lr3/\n",
      "Running model: cnn_lay2_drop3_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6971 - acc: 0.5938 - val_loss: 0.6892 - val_acc: 0.5150\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop3_lr6/\n",
      "Running model: cnn_lay2_drop3_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.6933 - acc: 0.5625 - val_loss: 0.7022 - val_acc: 0.4800\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop6_lr0/\n",
      "Running model: cnn_lay2_drop6_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7186 - acc: 0.3750 - val_loss: 8.2069 - val_acc: 0.4900\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop6_lr3/\n",
      "Running model: cnn_lay2_drop6_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6791 - acc: 0.5938 - val_loss: 0.6969 - val_acc: 0.4800\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay2_drop6_lr6/\n",
      "Running model: cnn_lay2_drop6_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.7025 - acc: 0.4688 - val_loss: 0.6936 - val_acc: 0.4900\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop0_lr0/\n",
      "Running model: cnn_lay4_drop0_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7025 - acc: 0.3125 - val_loss: 1.8560 - val_acc: 0.4850\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop0_lr3/\n",
      "Running model: cnn_lay4_drop0_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7130 - acc: 0.3125 - val_loss: 0.6892 - val_acc: 0.5150\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop0_lr6/\n",
      "Running model: cnn_lay4_drop0_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6908 - acc: 0.5000 - val_loss: 0.6928 - val_acc: 0.4650\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop3_lr0/\n",
      "Running model: cnn_lay4_drop3_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6925 - acc: 0.4688 - val_loss: 0.7933 - val_acc: 0.5200\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop3_lr3/\n",
      "Running model: cnn_lay4_drop3_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6955 - acc: 0.4375 - val_loss: 0.6932 - val_acc: 0.5050\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop3_lr6/\n",
      "Running model: cnn_lay4_drop3_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6933 - acc: 0.5000 - val_loss: 0.6935 - val_acc: 0.4850\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop6_lr0/\n",
      "Running model: cnn_lay4_drop6_lr0\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6996 - acc: 0.5625 - val_loss: 1.2222 - val_acc: 0.5550\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop6_lr3/\n",
      "Running model: cnn_lay4_drop6_lr3\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7013 - acc: 0.4688 - val_loss: 0.6915 - val_acc: 0.5350\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/cnn_lay4_drop6_lr6/\n",
      "Running model: cnn_lay4_drop6_lr6\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6881 - acc: 0.5312 - val_loss: 0.6892 - val_acc: 0.5400\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/vgg_trainable_aug/\n",
      "Running model: vgg_trainable_aug\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 24s 24s/step - loss: 0.6925 - acc: 0.4688 - val_loss: 0.7137 - val_acc: 0.4950\n",
      "\tSaving model...\n",
      "Results directory: /home/jupyter/models/vgg_fixed_aug/\n",
      "Running model: vgg_fixed_aug\n",
      "\tDefining generators...\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "\tFitting model...\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7505 - acc: 0.3750 - val_loss: 0.6855 - val_acc: 0.5800\n",
      "\tSaving model...\n"
     ]
    }
   ],
   "source": [
    "for irow in models:\n",
    "    single_iter(irow, run_now = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_class(class_name, num_samples):\n",
    "    '''\n",
    "    Return a list of images from a desired class\n",
    "    '''\n",
    "    offset = 0 \n",
    "    class_path  = os.path.join(data_folder, 'test', class_name)\n",
    "    all_image_paths = [os.path.join(class_path, ifile) for ifile in os.listdir(class_path) if ifile != '.DS_Store']  \n",
    "    image_paths = [all_image_paths[isample] for isample in range(offset, num_samples + offset)]\n",
    "\n",
    "    images = []\n",
    "#     i = 0\n",
    "    for ipath in image_paths:\n",
    "#         i +=1\n",
    "#         print(i)\n",
    "        image = cv2.imread(ipath)/255.0\n",
    "        images.append(image)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_dataframe():\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    all_image_paths   = glob.glob(os.path.join(data_folder,'*','*','*.jpeg'))\n",
    "    all_image_classes = [ipath.split('/')[-2] for ipath in all_image_paths]\n",
    "    all_image_splits  = [ipath.split('/')[-3] for ipath in all_image_paths]\n",
    "    all_meta_data = {'class': all_image_classes, 'image_path':all_image_paths, 'split': all_image_splits}\n",
    "    return pd.DataFrame(all_meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = get_meta_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">test</th>\n",
       "      <th>NORMAL</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PNEUMONIA</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">train</th>\n",
       "      <th>NORMAL</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PNEUMONIA</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">val</th>\n",
       "      <th>NORMAL</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PNEUMONIA</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_path\n",
       "split class                \n",
       "test  NORMAL            200\n",
       "      PNEUMONIA         200\n",
       "train NORMAL           1000\n",
       "      PNEUMONIA        1000\n",
       "val   NORMAL              8\n",
       "      PNEUMONIA           8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.groupby(['split','class']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal = get_images_from_class('NORMAL',200)\n",
    "normal_labels = [1 for i in range(200)]\n",
    "sick = get_images_from_class('PNEUMONIA',200)\n",
    "sick_labels = [0 for i in range(200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_images = normal + sick\n",
    "all_labels = normal_labels + sick_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_test = np.stack(all_images, axis = 3).reshape(-1, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test = np.stack(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg_model.fit_generator(datagen.flow(x_train, y_train, batch_size = 32), steps_per_epoch=121, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import cv2\n",
    "\n",
    "#@title Execute this code block! This will prepare a handful of useful functions for you to use, described below\n",
    "set_labels   = ['test','val','train']\n",
    "class_labels = ['PNEUMONIA','NORMAL']\n",
    "class_mode   = 'binary'\n",
    "image_size   = (224, 224)\n",
    "\n",
    "def get_train_data_generator(augment = False, color = 'rgb'):\n",
    "    '''\n",
    "    Returns a Python generator (see intro Python lecture) that returns an image\n",
    "    that has gone through the data augmentation process from the training set.\n",
    "    '''\n",
    "\n",
    "    # Data augmentation for training dataset.\n",
    "    if not augment:\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    else:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "              rescale=1./255,\n",
    "              shear_range=0.2,\n",
    "              zoom_range=0.2,\n",
    "              horizontal_flip=True)\n",
    "\n",
    "    # Create a Python 'generator' for reading pictures from \n",
    "    # the 'Datasets/chest_xray/train' folder, and indefinitely \n",
    "    # generate batches of augmented image data.\n",
    "    image_directory = os.path.join(data_folder, 'train')  \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "          image_directory, \n",
    "          target_size=image_size,\n",
    "          batch_size=32,\n",
    "          color_mode=color, # depends on the dataset\n",
    "          class_mode=class_mode)    \n",
    "\n",
    "    return train_generator\n",
    "\n",
    "def get_test_data_generator(augment = False, color = 'rgb'):\n",
    "    '''\n",
    "    Returns a Python generator (see intro Python lecture) that returns an image\n",
    "    that has gone through the data augmentation process from the training set.\n",
    "    '''\n",
    "\n",
    "    # Data augmentation for training dataset.\n",
    "    if not augment:\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    else:\n",
    "        test_datagen = ImageDataGenerator(\n",
    "              rescale=1./255,\n",
    "              shear_range=0.2,\n",
    "              zoom_range=0.2,\n",
    "              horizontal_flip=True)\n",
    "\n",
    "    # Create a Python 'generator' for reading pictures from \n",
    "    # the 'Datasets/chest_xray/train' folder, and indefinitely \n",
    "    # generate batches of augmented image data.\n",
    "    image_directory = os.path.join(data_folder, 'test')  \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "          image_directory, \n",
    "          target_size=image_size,\n",
    "          batch_size=200,\n",
    "          color_mode=color, # depends on the dataset\n",
    "          class_mode=class_mode)    \n",
    "\n",
    "    return test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we build a logistic regression \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(224, 224, 3)))\n",
    "model.add(Dense(1, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and we hand our model the 'training book to learn from'\n",
    "model.fit_generator(get_train_data_generator(color = 'rgb'),\n",
    "                    validation_data = get_test_data_generator(color = 'rgb'),\n",
    "                    validation_steps = 1,\n",
    "                    steps_per_epoch = 121,\n",
    "                    epochs = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "\n",
    "cnn_model.add(Conv2D(64, (3, 3), input_shape=(224, 224, 3)))\n",
    "cnn_model.add(Activation('relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn_model.add(Flatten()) \n",
    "\n",
    "cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.95),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-ba14a8d30e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m121\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     callbacks=[checkpoint, csv_logger]) \n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('cnn_model{epoch:08d}.h5', period=1) \n",
    "csv_logger = CSVLogger(\"cnn_model_history_log.csv\", append=True)\n",
    "\n",
    "# and we hand our model the 'training book to learn from'\n",
    "cnn_model.fit_generator(get_train_data_generator(color = 'rgb'),\n",
    "                    validation_data = get_test_data_generator(color = 'rgb'),\n",
    "                    validation_steps = 1,\n",
    "                    steps_per_epoch = 121,\n",
    "                    epochs = 10, \n",
    "                    callbacks=[checkpoint, csv_logger]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model look up table\n",
    "\n",
    "# base model function \n",
    "# fcc\n",
    "# cnn(number_layers, dropout, learning_rate, uses_augmentation )\n",
    "# 2, 4, 6\n",
    "# 0, 0.3, 0.6\n",
    "# learning_rate: -6, -4, -2\n",
    "\n",
    "\n",
    "# for the best one, check against augmentation \n",
    "    # uses augmentation, doesn't use augmentation \n",
    "\n",
    "2*3*4 = \n",
    "# vgg()\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_drop = Sequential()\n",
    "\n",
    "cnn_model_drop.add(Conv2D(64, (3, 3), input_shape=(224, 224, 3)))\n",
    "cnn_model_drop.add(Activation('relu'))\n",
    "cnn_model_drop.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn_model_drop.add(Flatten()) \n",
    "\n",
    "cnn_model_drop.add(Dense(units = 128, activation = 'relu'))\n",
    "cnn_model_drop.add(Dropout(0.5))\n",
    "cnn_model_drop.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "cnn_model_drop.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.95),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Tensor conv2d_3_input:0, specified in either feed_devices or fetch_devices was not found in the Graph",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-78ec4f5c5d77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m121\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     epochs = 10) \n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Tensor conv2d_3_input:0, specified in either feed_devices or fetch_devices was not found in the Graph"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# and we hand our model the 'training book to learn from'\n",
    "cnn_model_drop.fit_generator(get_train_data_generator(color = 'rgb'),\n",
    "                    validation_data = get_test_data_generator(color = 'rgb'),\n",
    "                    validation_steps = 1,\n",
    "                    steps_per_epoch = 121,\n",
    "                    epochs = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "121/121 [==============================] - 22s 184ms/step - loss: 0.5310 - acc: 0.7454 - val_loss: 0.4479 - val_acc: 0.8350\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 20s 164ms/step - loss: 0.3048 - acc: 0.8926 - val_loss: 0.3814 - val_acc: 0.8200\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 20s 165ms/step - loss: 0.2480 - acc: 0.9117 - val_loss: 0.4874 - val_acc: 0.7400\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 20s 165ms/step - loss: 0.2081 - acc: 0.9241 - val_loss: 0.4192 - val_acc: 0.7600\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 20s 165ms/step - loss: 0.1889 - acc: 0.9269 - val_loss: 0.4231 - val_acc: 0.7850\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 20s 165ms/step - loss: 0.1907 - acc: 0.9241 - val_loss: 0.4432 - val_acc: 0.7500\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 20s 164ms/step - loss: 0.2000 - acc: 0.9264 - val_loss: 0.6726 - val_acc: 0.7150\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 20s 164ms/step - loss: 0.1615 - acc: 0.9411 - val_loss: 0.6382 - val_acc: 0.6900\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 20s 164ms/step - loss: 0.1828 - acc: 0.9305 - val_loss: 0.7217 - val_acc: 0.7150\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 20s 163ms/step - loss: 0.1784 - acc: 0.9349 - val_loss: 0.9932 - val_acc: 0.6050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef31384ba8>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we hand our model the 'training book to learn from'\n",
    "cnn_model_drop.fit_generator(get_train_data_generator(color = 'rgb'),\n",
    "                    validation_data = get_test_data_generator(color = 'rgb'),\n",
    "                    validation_steps = 1,\n",
    "                    steps_per_epoch = 121,\n",
    "                    epochs = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the training curve - overfitting to the training data\n",
    "cnn_model.save('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = tf.keras.models.load_model('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.7946058091286307,\n",
       "  0.9114583333333334,\n",
       "  0.9317708333333333,\n",
       "  0.9471354166666667,\n",
       "  0.9505208333333334,\n",
       "  0.9565104166666667,\n",
       "  0.9544270833333334,\n",
       "  0.9565104166666667,\n",
       "  0.9447916666666667,\n",
       "  0.9546875],\n",
       " 'loss': [0.4549224703143741,\n",
       "  0.2428989332790176,\n",
       "  0.1750654386356473,\n",
       "  0.14736155569553375,\n",
       "  0.1354918708248685,\n",
       "  0.12667551067036886,\n",
       "  0.11829052766164144,\n",
       "  0.11442604120820761,\n",
       "  0.1384646851569414,\n",
       "  0.12114966735243797],\n",
       " 'val_acc': [0.8500000238418579,\n",
       "  0.8100000023841858,\n",
       "  0.7549999952316284,\n",
       "  0.7300000190734863,\n",
       "  0.7200000286102295,\n",
       "  0.7200000286102295,\n",
       "  0.7200000286102295,\n",
       "  0.7049999833106995,\n",
       "  0.7450000047683716,\n",
       "  0.800000011920929],\n",
       " 'val_loss': [0.38410472869873047,\n",
       "  0.3845076858997345,\n",
       "  0.4405522644519806,\n",
       "  0.6263845562934875,\n",
       "  0.5592684149742126,\n",
       "  0.6501896381378174,\n",
       "  0.7666110992431641,\n",
       "  0.8493646383285522,\n",
       "  0.5506953001022339,\n",
       "  0.4264497458934784]}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trainedmodel_50Epoch.h5\") # saving the model\n",
    "with open('trainHistoryOld', 'wb') as handle: # saving the history of the model\n",
    "    dump(history.history, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "# load vgg 16 \n",
    "vgg_conv = VGG16(weights = 'imagenet', \n",
    "     include_top = False, \n",
    "     input_shape = (224, 224, 3))\n",
    "\n",
    "for layer in vgg_conv.layers:\n",
    "    layer.trainable = True\n",
    "  \n",
    "vgg_model = Sequential()\n",
    "out_vgg   = vgg_conv # GlobalAveragePooling2D()(vgg_conv.output)\n",
    "vgg_model.add(out_vgg)\n",
    "vgg_model.add(GlobalAveragePooling2D())\n",
    "vgg_model.add(Dense(1024, activation = 'relu'))\n",
    "vgg_model.add(Dropout(0.5))\n",
    "vgg_model.add(Dense(512, activation = 'relu'))\n",
    "vgg_model.add(Dropout(0.5))\n",
    "\n",
    "vgg_model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('vgg_model{epoch:08d}.h5', period=1) \n",
    "csv_logger = CSVLogger(\"vgg_model_history_log.csv\", append=True)\n",
    "\n",
    "vgg_model.compile(loss = 'binary_crossentropy', \n",
    "              optimizer = optimizers.SGD(lr=1e-4, momentum=0.95), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# https://forums.fast.ai/t/how-to-use-pre-trained-features-from-vgg-16-as-input-to-globalaveragepooling2d-layer-in-keras/3196/3\n",
    "\n",
    "vgg_model.fit_generator(get_train_data_generator(color = 'rgb'), \n",
    "                        validation_data=get_test_data_generator(color = 'rgb'), \n",
    "                        validation_steps =1, \n",
    "                        steps_per_epoch = 121, \n",
    "                        epochs = 1,\n",
    "                        callbacks=[checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.5, 0.4895833333333333, 0.46875, 0.5625, 0.40625],\n",
       " 'loss': [7.971192836761475,\n",
       "  8.137259324391684,\n",
       "  8.469392458597818,\n",
       "  6.974793593088786,\n",
       "  9.465791384379068]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.history.history # loss is saved after each epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/kosovanolexandr/keras-nn-x-ray-predict-pneumonia-86-54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binary classification with Keras neural network\n",
    "English is not my native language, so sorry for any mistake.\n",
    "\n",
    "If you like my Kernel, give me some feedback and also votes up my kernel.\n",
    "\n",
    "Import\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
